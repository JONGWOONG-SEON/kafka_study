

영속성
- 영속성이란 데이터를 생성한 프로그램이 종료되더라도 사라지지 않은 데이터의 특성을 뜻함
- 카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장
- 파일 시스템에 저장하는 것은 보편적으로 느리다고 생각하겠지만, 카프카는 운영체제 레벨에서 파일 시스템을 최대한 활용하는 방법을 적용
- 운영체제에서는 파일 I/O 성능 향상을 위해 페이지 캐시(page cache) 영역을 메모리에 따로 생성하여 사용
- 페이지 캐시 메모리 영역을 사용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식
- 그렇기 때문에 카프카가 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높은 것.
- 디스크 기반의 파일 시스템을 활용한 덕분에 브로커 애플리케이션이 장애 발생으로 인해 급작스럽게 종료되더라도 프로세스를 재시작하여 안전하게 데이터를 다시 처리할 수 있음


카프카 브로커 힙 메모리 설정

- 카프카 브로커를 실행하기 위해서는 힙 메모리 설정이 필요함. 
- 카프카 브로커는 레코드의 내용은 페이지 캐시로 시스템 메모리를 사용하고 나머지 객체들을 힙 메모리에 저장하여 사용한다는 특징이 있음. 
- 이러한 특징으로 카프카 브로커를 운영할 때 힙 메모리를 5GB 이상으로 설정하지 않는 것이 일반적


min.insync.replicas: 2  # 최소 2개 이상의 브로커에 데이터가 완전히 복제됨을 보장함


각종 kakfa 설정 정보
```shell
docker exec -it confluent_community_zookeeper_kafka-kafka-1 bash
cd /etc/kafka
```

listeners
- https://stackoverflow.com/questions/73849828/what-is-mean-plaintext-host

하드웨어 설정 정보
https://kubernetes.io/ko/docs/tasks/administer-cluster/sysctl-cluster/#%EB%AA%A8%EB%93%A0-sysctl-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EB%82%98%EC%97%B4
```shell
sudo sysctl -a
```



```shell
kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic myTopic
```

```shell
Topic:myTopic       PartitionCount:4        ReplicationFactor:1     Configs:
    Topic: myTopic      Partition: 0    Leader: 2       Replicas: 2     Isr: 2
    Topic: myTopic      Partition: 1    Leader: 3       Replicas: 3     Isr: 3
    Topic: myTopic      Partition: 2    Leader: 4       Replicas: 4     Isr: 4
    Topic: myTopic      Partition: 3    Leader: 0       Replicas: 0     Isr: 0
```
```markdown
Topic: myTopic은 Kafka 토픽의 이름입니다.
PartitionCount: 4는 이 토픽에 4개의 파티션이 있다는 것을 의미합니다.
ReplicationFactor: 1은 각 파티션이 1개의 복제본만 가지고 있다는 것을 나타냅니다.
이 경우, 복제본이 1개이므로 리더와 복제본이 동일합니다.
Configs: 현재 이 토픽에 설정된 추가적인 설정이 없습니다.

각 필드의 의미:
Topic: 토픽 이름 (모두 myTopic으로 동일).
Partition: 파티션 번호. 각 토픽은 여러 파티션으로 나뉘며, 여기선 0, 1, 2, 3의 4개 파티션이 있습니다.
Leader: 해당 파티션의 리더 역할을 맡고 있는 브로커의 ID.
예: 파티션 0의 리더는 브로커 2입니다.
Replicas: 해당 파티션의 **복제본(replica)**을 저장하는 브로커 ID.
ReplicationFactor가 1이므로, 리더와 동일합니다.
Isr: In-Sync Replica, 즉 리더와 동기화된 상태인 복제본.
여기선 ReplicationFactor가 1이라 항상 리더와 동일.


리더는 브로커 3.
복제본은 브로커 3에만 있음.
동기화된 복제본(ISR)도 브로커 3.


왜 이런 상태인지?
ReplicationFactor가 1인 이유:

이 설정에서는 각 파티션에 하나의 복제본만 존재합니다. 따라서 장애 허용(fault-tolerance)이 없습니다.
리더와 복제본이 동일한 브로커에서 관리됩니다.
각 브로커가 하나의 리더를 담당:

Kafka는 파티션의 리더를 브로커에 분산시켜 워크로드를 나눕니다.
여기서 브로커 2, 3, 4, 0이 각각 하나의 리더를 담당하고 있습니다.




```


```shell
[appuser@0d2778b18f41 bin]$ kafka-topics --describe --bootstrap-server kafka:9092
Topic: user-topic       TopicId: CoD6JEG7QuyTjE3F-iXEcA PartitionCount: 1       ReplicationFactor: 1    Configs: 
        Topic: user-topic       Partition: 0    Leader: 1       Replicas: 1     Isr: 1
Topic: __consumer_offsets       TopicId: wfyyqIbTR7uVS3Gph5XWyw PartitionCount: 50      ReplicationFactor: 1    Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600
        Topic: __consumer_offsets       Partition: 0    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 1    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 2    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 3    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 4    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 5    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 6    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 7    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 8    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 9    Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 10   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 11   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 12   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 13   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 14   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 15   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 16   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 17   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 18   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 19   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 20   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 21   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 22   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 23   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 24   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 25   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 26   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 27   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 28   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 29   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 30   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 31   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 32   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 33   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 34   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 35   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 36   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 37   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 38   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 39   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 40   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 41   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 42   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 43   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 44   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 45   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 46   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 47   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 48   Leader: 1       Replicas: 1     Isr: 1
        Topic: __consumer_offsets       Partition: 49   Leader: 1       Replicas: 1     Isr: 1
Topic: _schemas TopicId: Kivx-29IQbOBwVjMc55jew PartitionCount: 1       ReplicationFactor: 1    Configs: cleanup.policy=compact
        Topic: _schemas Partition: 0    Leader: 1       Replicas: 1     Isr: 1
```


```shell
[appuser@0d2778b18f41 bin]$ kafka-broker-api-versions --bootstrap-server kafka:9092
kafka:9092 (id: 1 rack: null) -> (
        Produce(0): 0 to 10 [usable: 10],
        Fetch(1): 0 to 16 [usable: 16],
        ListOffsets(2): 0 to 8 [usable: 8],
        Metadata(3): 0 to 12 [usable: 12],
        LeaderAndIsr(4): 0 to 7 [usable: 7],
        StopReplica(5): 0 to 4 [usable: 4],
        UpdateMetadata(6): 0 to 8 [usable: 8],
        ControlledShutdown(7): 0 to 3 [usable: 3],
        OffsetCommit(8): 0 to 9 [usable: 9],
        OffsetFetch(9): 0 to 9 [usable: 9],
        FindCoordinator(10): 0 to 4 [usable: 4],
        JoinGroup(11): 0 to 9 [usable: 9],
        Heartbeat(12): 0 to 4 [usable: 4],
        LeaveGroup(13): 0 to 5 [usable: 5],
        SyncGroup(14): 0 to 5 [usable: 5],
        DescribeGroups(15): 0 to 5 [usable: 5],
        ListGroups(16): 0 to 4 [usable: 4],
        SaslHandshake(17): 0 to 1 [usable: 1],
        ApiVersions(18): 0 to 3 [usable: 3],
        CreateTopics(19): 0 to 7 [usable: 7],
        DeleteTopics(20): 0 to 6 [usable: 6],
        DeleteRecords(21): 0 to 2 [usable: 2],
        InitProducerId(22): 0 to 4 [usable: 4],
        OffsetForLeaderEpoch(23): 0 to 4 [usable: 4],
        AddPartitionsToTxn(24): 0 to 4 [usable: 4],
        AddOffsetsToTxn(25): 0 to 3 [usable: 3],
        EndTxn(26): 0 to 3 [usable: 3],
        WriteTxnMarkers(27): 0 to 1 [usable: 1],
        TxnOffsetCommit(28): 0 to 3 [usable: 3],
        DescribeAcls(29): 0 to 3 [usable: 3],
        CreateAcls(30): 0 to 3 [usable: 3],
        DeleteAcls(31): 0 to 3 [usable: 3],
        DescribeConfigs(32): 0 to 4 [usable: 4],
        AlterConfigs(33): 0 to 2 [usable: 2],
        AlterReplicaLogDirs(34): 0 to 2 [usable: 2],
        DescribeLogDirs(35): 0 to 4 [usable: 4],
        SaslAuthenticate(36): 0 to 2 [usable: 2],
        CreatePartitions(37): 0 to 3 [usable: 3],
        CreateDelegationToken(38): 0 to 3 [usable: 3],
        RenewDelegationToken(39): 0 to 2 [usable: 2],
        ExpireDelegationToken(40): 0 to 2 [usable: 2],
        DescribeDelegationToken(41): 0 to 3 [usable: 3],
        DeleteGroups(42): 0 to 2 [usable: 2],
        ElectLeaders(43): 0 to 2 [usable: 2],
        IncrementalAlterConfigs(44): 0 to 1 [usable: 1],
        AlterPartitionReassignments(45): 0 [usable: 0],
        ListPartitionReassignments(46): 0 [usable: 0],
        OffsetDelete(47): 0 [usable: 0],
        DescribeClientQuotas(48): 0 to 1 [usable: 1],
        AlterClientQuotas(49): 0 to 1 [usable: 1],
        DescribeUserScramCredentials(50): 0 [usable: 0],
        AlterUserScramCredentials(51): 0 [usable: 0],
        DescribeQuorum(55): UNSUPPORTED,
        AlterPartition(56): 0 to 3 [usable: 3],
        UpdateFeatures(57): 0 to 1 [usable: 1],
        Envelope(58): 0 [usable: 0],
        DescribeCluster(60): 0 to 1 [usable: 1],
        DescribeProducers(61): 0 [usable: 0],
        UnregisterBroker(64): UNSUPPORTED,
        DescribeTransactions(65): 0 [usable: 0],
        ListTransactions(66): 0 [usable: 0],
        AllocateProducerIds(67): 0 [usable: 0],
        ConsumerGroupHeartbeat(68): 0 [usable: 0],
        ConsumerGroupDescribe(69): UNSUPPORTED,
        GetTelemetrySubscriptions(71): UNSUPPORTED,
        PushTelemetry(72): UNSUPPORTED,
        ListClientMetricsResources(74): UNSUPPORTED
)
```

#### min.insync.replicas
```markdown
min.insync.replicas는 프로듀서가 메시지를 커밋(acknowledge)할 때, 복제본 중 몇 개의 ISR(In-Sync Replicas)이 커밋에 참여해야 하는지를 나타냄
이 값은 replication.factor보다 항상 작거나 같아야 합니다.
Replication Factor = 1인 경우, ISR에 리더만 존재하므로 min.insync.replicas = 1이어야 합니다.
만약 min.insync.replicas > 1로 설정되면, 메시지 커밋이 실패합니다. (ISR 내 복제본이 부족하기 때문)
```

브로커 레벨 ISR
토픽 레벨 ISR

lag
replica.lag.time.max.ms

lag burrow
https://blog.voidmainvoid.net/279

https://jivebreaddev.gitbook.io/jivebreaddevlog/conference/2024-kafka-krug

```markdown
추가적인 질문들이 이런 식으로 생겼다.

리더 공백에 대해서는 다른 시스템에서는 어떻게 처리하는지?

Traffic Throttling으로 리밸런싱동안 대역폭 제한하는 기능이 있다.

클라이언트 쪽에서 backoff 전략을 사용하고 리밸런싱 동안 잠시 Producing을 멈추는 방법도 있어보인다.

카프카가 Network 대역폭을 어떻게 사용하고 DISK I/O를 어떻게 소모하는지

kafka에서 메시지가 오고 page cache 에 저장하고 consumer에서 page cache에서 읽으려고 하다가 없으면 disk에서 읽어온다

log.flush.interval.messages, ms 라는 옵션이 있다.
```

replica.lag.time.max.ms	
팔로워가 가져오기 요청을 전송하지 않았거나 최소한 이 밀리초 동안 리더의 로그 종료 오프셋까지 사용하지 않은 경우 리더는 에서 팔로워를 제거합니다  ISR.


https://developnote-blog.tistory.com/182

https://velog.io/@tedigom/Kafka-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-Rebalance-%ED%95%98%EA%B8%B0
https://velog.io/@ddongh1122/Kafka-%EC%9D%B4%ED%95%B4-%EC%B9%B4%ED%94%84%EC%B9%B4-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90-3


skew
https://louisdev.tistory.com/14

skew 게산 방식
skew로 초래되는 문제
skew 해결 방식


leader_epoch 
https://chatgpt.com/share/6746ebd8-e574-8009-a59d-a8bf8b87f02e
https://colevelup.tistory.com/20  # leader_epoch 

```shell
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,771] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)

```


```shell
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-34. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-30. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-27. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-39. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-6. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-3. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-36. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,594] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-33. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,692] INFO [GroupCoordinator 2]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-4a403775-03c3-4104-b4f1-b041136c5567 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
2024-11-26 18:04:20 [2024-11-26 09:04:20,696] INFO [GroupCoordinator 2]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-4a403775-03c3-4104-b4f1-b041136c5567 with group instance id None; client reason: need to re-join with the given member-id: sr-1-4a403775-03c3-4104-b4f1-b041136c5567) (kafka.coordinator.group.GroupCoordinator)
2024-11-26 18:04:23 [2024-11-26 09:04:23,107] INFO [Controller id=2] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,108] TRACE [Controller id=2] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,111] DEBUG [Controller id=2] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,112] TRACE [Controller id=2] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,112] DEBUG [Controller id=2] Topics not in preferred replica for broker 2 HashMap() (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,112] TRACE [Controller id=2] Leader imbalance ratio for broker 2 is 0.0 (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,112] DEBUG [Controller id=2] Topics not in preferred replica for broker 3 HashMap() (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,112] TRACE [Controller id=2] Leader imbalance ratio for broker 3 is 0.0 (kafka.controller.KafkaController)
2024-11-26 18:04:23 [2024-11-26 09:04:23,700] INFO [GroupCoordinator 2]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
2024-11-26 18:04:23 [2024-11-26 09:04:23,713] INFO [GroupCoordinator 2]: Assignment received from leader sr-1-4a403775-03c3-4104-b4f1-b041136c5567 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
```

```shell
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-21. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-18. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-30. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-27. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-39. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-6. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-3. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-36. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,591] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition __consumer_offsets-33. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,772] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,772] INFO [UnifiedLog partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,772] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0 (kafka.server.ReplicaFetcherThread)
2024-11-26 18:04:20 [2024-11-26 09:04:20,773] INFO [UnifiedLog partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.UnifiedLog)
```

```shell
ava/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.54.v20240208.jar:/usr/bin/../share/java/kafka/jersey-common-2.39.1.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.8.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.6-4.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.9.2.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.54.v20240208.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.10.0.jar:/usr/bin/../share/java/kafka/protobuf-java-3.23.4.jar:/usr/bin/../share/java/kafka/reflections-0.10.2.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.7.1-ccs.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/connect-json-7.7.1-ccs.jar:/usr/bin/../share/java/kafka/commons-validator-1.7.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.16.2.jar:/usr/bin/../share/java/kafka/opentelemetry-proto-1.0.0-alpha.jar:/usr/bin/../share/java/kafka/jackson-core-2.16.2.jar:/usr/bin/../share/java/kafka/javassist-3.29.2-GA.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.7.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-clients-7.7.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.1.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-tools-7.7.1-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:os.arch=aarch64 (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:os.version=6.10.11-linuxkit (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:os.memory.free=987MB (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,171] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,172] INFO Initiating client connection, connectString=zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@6aa8e115 (org.apache.zookeeper.ZooKeeper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,175] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
2024-11-26 18:35:37 [2024-11-26 09:35:37,178] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
2024-11-26 18:35:37 [2024-11-26 09:35:37,179] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
2024-11-26 18:35:37 [2024-11-26 09:35:37,180] INFO Opening socket connection to server zookeeper-1/172.21.0.5:2181. (org.apache.zookeeper.ClientCnxn)
2024-11-26 18:35:37 [2024-11-26 09:35:37,181] INFO Socket connection established, initiating session, client: /172.21.0.8:53828, server: zookeeper-1/172.21.0.5:2181 (org.apache.zookeeper.ClientCnxn)
2024-11-26 18:35:37 [2024-11-26 09:35:37,190] INFO Session establishment complete on server zookeeper-1/172.21.0.5:2181, session id = 0x100000015110000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
2024-11-26 18:35:37 [2024-11-26 09:35:37,192] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
2024-11-26 18:35:37 [2024-11-26 09:35:37,330] INFO Cluster ID = X4DTW78eTUiazIPCiikLwg (kafka.server.KafkaServer)
2024-11-26 18:35:37 [2024-11-26 09:35:37,354] INFO KafkaConfig values: 
2024-11-26 18:35:37     advertised.listeners = PLAINTEXT://kafka-2:9092,PLAINTEXT_HOST://localhost:39092
2024-11-26 18:35:37     alter.config.policy.class.name = null
2024-11-26 18:35:37     alter.log.dirs.replication.quota.window.num = 11
2024-11-26 18:35:37     alter.log.dirs.replication.quota.window.size.seconds = 1
2024-11-26 18:35:37     authorizer.class.name = 
2024-11-26 18:35:37     auto.create.topics.enable = true
2024-11-26 18:35:37     auto.include.jmx.reporter = true
2024-11-26 18:35:37     auto.leader.rebalance.enable = true
2024-11-26 18:35:37     background.threads = 10
2024-11-26 18:35:37     broker.heartbeat.interval.ms = 2000
2024-11-26 18:35:37     broker.id = 2
2024-11-26 18:35:37     broker.id.generation.enable = true
2024-11-26 18:35:37     broker.rack = null
2024-11-26 18:35:37     broker.session.timeout.ms = 9000
2024-11-26 18:35:37     client.quota.callback.class = null
2024-11-26 18:35:37     compression.type = producer
2024-11-26 18:35:37     connection.failed.authentication.delay.ms = 100
2024-11-26 18:35:37     connections.max.idle.ms = 600000
2024-11-26 18:35:37     connections.max.reauth.ms = 0
2024-11-26 18:35:37     control.plane.listener.name = null
2024-11-26 18:35:37     controlled.shutdown.enable = true
2024-11-26 18:35:37     controlled.shutdown.max.retries = 3
2024-11-26 18:35:37     controlled.shutdown.retry.backoff.ms = 5000
2024-11-26 18:35:37     controller.listener.names = null
2024-11-26 18:35:37     controller.quorum.append.linger.ms = 25
2024-11-26 18:35:37     controller.quorum.election.backoff.max.ms = 1000
2024-11-26 18:35:37     controller.quorum.election.timeout.ms = 1000
2024-11-26 18:35:37     controller.quorum.fetch.timeout.ms = 2000
2024-11-26 18:35:37     controller.quorum.request.timeout.ms = 2000
2024-11-26 18:35:37     controller.quorum.retry.backoff.ms = 20
2024-11-26 18:35:37     controller.quorum.voters = []
2024-11-26 18:35:37     controller.quota.window.num = 11
2024-11-26 18:35:37     controller.quota.window.size.seconds = 1
2024-11-26 18:35:37     controller.socket.timeout.ms = 30000
2024-11-26 18:35:37     create.topic.policy.class.name = null
2024-11-26 18:35:37     default.replication.factor = 1
2024-11-26 18:35:37     delegation.token.expiry.check.interval.ms = 3600000
2024-11-26 18:35:37     delegation.token.expiry.time.ms = 86400000
2024-11-26 18:35:37     delegation.token.master.key = null
2024-11-26 18:35:37     delegation.token.max.lifetime.ms = 604800000
2024-11-26 18:35:37     delegation.token.secret.key = null
2024-11-26 18:35:37     delete.records.purgatory.purge.interval.requests = 1
2024-11-26 18:35:37     delete.topic.enable = true
2024-11-26 18:35:37     early.start.listeners = null
2024-11-26 18:35:37     eligible.leader.replicas.enable = false
2024-11-26 18:35:37     fetch.max.bytes = 57671680
2024-11-26 18:35:37     fetch.purgatory.purge.interval.requests = 1000
2024-11-26 18:35:37     group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
2024-11-26 18:35:37     group.consumer.heartbeat.interval.ms = 5000
2024-11-26 18:35:37     group.consumer.max.heartbeat.interval.ms = 15000
2024-11-26 18:35:37     group.consumer.max.session.timeout.ms = 60000
2024-11-26 18:35:37     group.consumer.max.size = 2147483647
2024-11-26 18:35:37     group.consumer.min.heartbeat.interval.ms = 5000
2024-11-26 18:35:37     group.consumer.min.session.timeout.ms = 45000
2024-11-26 18:35:37     group.consumer.session.timeout.ms = 45000
2024-11-26 18:35:37     group.coordinator.new.enable = false
2024-11-26 18:35:37     group.coordinator.rebalance.protocols = [classic]
2024-11-26 18:35:37     group.coordinator.threads = 1
2024-11-26 18:35:37     group.initial.rebalance.delay.ms = 3000
2024-11-26 18:35:37     group.max.session.timeout.ms = 1800000
2024-11-26 18:35:37     group.max.size = 2147483647
2024-11-26 18:35:37     group.min.session.timeout.ms = 6000
2024-11-26 18:35:37     initial.broker.registration.timeout.ms = 60000
2024-11-26 18:35:37     inter.broker.listener.name = PLAINTEXT
2024-11-26 18:35:37     inter.broker.protocol.version = 3.7-IV4
2024-11-26 18:35:37     kafka.metrics.polling.interval.secs = 10
2024-11-26 18:35:37     kafka.metrics.reporters = []
2024-11-26 18:35:37     leader.imbalance.check.interval.seconds = 300
2024-11-26 18:35:37     leader.imbalance.per.broker.percentage = 10
2024-11-26 18:35:37     listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
2024-11-26 18:35:37     listeners = PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:39092
2024-11-26 18:35:37     log.cleaner.backoff.ms = 15000
2024-11-26 18:35:37     log.cleaner.dedupe.buffer.size = 134217728
2024-11-26 18:35:37     log.cleaner.delete.retention.ms = 86400000
2024-11-26 18:35:37     log.cleaner.enable = true
2024-11-26 18:35:37     log.cleaner.io.buffer.load.factor = 0.9
2024-11-26 18:35:37     log.cleaner.io.buffer.size = 524288
2024-11-26 18:35:37     log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
2024-11-26 18:35:37     log.cleaner.max.compaction.lag.ms = 9223372036854775807
2024-11-26 18:35:37     log.cleaner.min.cleanable.ratio = 0.5
2024-11-26 18:35:37     log.cleaner.min.compaction.lag.ms = 0
2024-11-26 18:35:37     log.cleaner.threads = 1
2024-11-26 18:35:37     log.cleanup.policy = [delete]
2024-11-26 18:35:37     log.dir = /tmp/kafka-logs
2024-11-26 18:35:37     log.dirs = /var/lib/kafka/data
2024-11-26 18:35:37     log.flush.interval.messages = 9223372036854775807
2024-11-26 18:35:37     log.flush.interval.ms = null
2024-11-26 18:35:37     log.flush.offset.checkpoint.interval.ms = 60000
2024-11-26 18:35:37     log.flush.scheduler.interval.ms = 9223372036854775807
2024-11-26 18:35:37     log.flush.start.offset.checkpoint.interval.ms = 60000
2024-11-26 18:35:37     log.index.interval.bytes = 4096
2024-11-26 18:35:37     log.index.size.max.bytes = 10485760
2024-11-26 18:35:37     log.local.retention.bytes = -2
2024-11-26 18:35:37     log.local.retention.ms = -2
2024-11-26 18:35:37     log.message.downconversion.enable = true
2024-11-26 18:35:37     log.message.format.version = 3.0-IV1
2024-11-26 18:35:37     log.message.timestamp.after.max.ms = 9223372036854775807
2024-11-26 18:35:37     log.message.timestamp.before.max.ms = 9223372036854775807
2024-11-26 18:35:37     log.message.timestamp.difference.max.ms = 9223372036854775807
2024-11-26 18:35:37     log.message.timestamp.type = CreateTime
2024-11-26 18:35:37     log.preallocate = false
2024-11-26 18:35:37     log.retention.bytes = -1
2024-11-26 18:35:37     log.retention.check.interval.ms = 300000
2024-11-26 18:35:37     log.retention.hours = 168
2024-11-26 18:35:37     log.retention.minutes = null
2024-11-26 18:35:37     log.retention.ms = null
2024-11-26 18:35:37     log.roll.hours = 168
2024-11-26 18:35:37     log.roll.jitter.hours = 0
2024-11-26 18:35:37     log.roll.jitter.ms = null
2024-11-26 18:35:37     log.roll.ms = null
2024-11-26 18:35:37     log.segment.bytes = 1073741824
2024-11-26 18:35:37     log.segment.delete.delay.ms = 60000
2024-11-26 18:35:37     max.connection.creation.rate = 2147483647
2024-11-26 18:35:37     max.connections = 2147483647
2024-11-26 18:35:37     max.connections.per.ip = 2147483647
2024-11-26 18:35:37     max.connections.per.ip.overrides = 
2024-11-26 18:35:37     max.incremental.fetch.session.cache.slots = 1000
2024-11-26 18:35:37     message.max.bytes = 1048588
2024-11-26 18:35:37     metadata.log.dir = null
2024-11-26 18:35:37     metadata.log.max.record.bytes.between.snapshots = 20971520
2024-11-26 18:35:37     metadata.log.max.snapshot.interval.ms = 3600000
2024-11-26 18:35:37     metadata.log.segment.bytes = 1073741824
2024-11-26 18:35:37     metadata.log.segment.min.bytes = 8388608
2024-11-26 18:35:37     metadata.log.segment.ms = 604800000
2024-11-26 18:35:37     metadata.max.idle.interval.ms = 500
2024-11-26 18:35:37     metadata.max.retention.bytes = 104857600
2024-11-26 18:35:37     metadata.max.retention.ms = 604800000
2024-11-26 18:35:37     metric.reporters = []
2024-11-26 18:35:37     metrics.num.samples = 2
2024-11-26 18:35:37     metrics.recording.level = INFO
2024-11-26 18:35:37     metrics.sample.window.ms = 30000
2024-11-26 18:35:37     min.insync.replicas = 1
2024-11-26 18:35:37     node.id = 2
2024-11-26 18:35:37     num.io.threads = 8
2024-11-26 18:35:37     num.network.threads = 3
2024-11-26 18:35:37     num.partitions = 1
2024-11-26 18:35:37     num.recovery.threads.per.data.dir = 1
2024-11-26 18:35:37     num.replica.alter.log.dirs.threads = null
2024-11-26 18:35:37     num.replica.fetchers = 1
2024-11-26 18:35:37     offset.metadata.max.bytes = 4096
2024-11-26 18:35:37     offsets.commit.required.acks = -1
2024-11-26 18:35:37     offsets.commit.timeout.ms = 5000
2024-11-26 18:35:37     offsets.load.buffer.size = 5242880
2024-11-26 18:35:37     offsets.retention.check.interval.ms = 600000
2024-11-26 18:35:37     offsets.retention.minutes = 10080
2024-11-26 18:35:37     offsets.topic.compression.codec = 0
2024-11-26 18:35:37     offsets.topic.num.partitions = 50
2024-11-26 18:35:37     offsets.topic.replication.factor = 3
2024-11-26 18:35:37     offsets.topic.segment.bytes = 104857600
2024-11-26 18:35:37     password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
2024-11-26 18:35:37     password.encoder.iterations = 4096
2024-11-26 18:35:37     password.encoder.key.length = 128
2024-11-26 18:35:37     password.encoder.keyfactory.algorithm = null
2024-11-26 18:35:37     password.encoder.old.secret = null
2024-11-26 18:35:37     password.encoder.secret = null
2024-11-26 18:35:37     principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
2024-11-26 18:35:37     process.roles = []
2024-11-26 18:35:37     producer.id.expiration.check.interval.ms = 600000
2024-11-26 18:35:37     producer.id.expiration.ms = 86400000
2024-11-26 18:35:37     producer.purgatory.purge.interval.requests = 1000
2024-11-26 18:35:37     queued.max.request.bytes = -1
2024-11-26 18:35:37     queued.max.requests = 500
2024-11-26 18:35:37     quota.window.num = 11
2024-11-26 18:35:37     quota.window.size.seconds = 1
2024-11-26 18:35:37     remote.log.index.file.cache.total.size.bytes = 1073741824
2024-11-26 18:35:37     remote.log.manager.task.interval.ms = 30000
2024-11-26 18:35:37     remote.log.manager.task.retry.backoff.max.ms = 30000
2024-11-26 18:35:37     remote.log.manager.task.retry.backoff.ms = 500
2024-11-26 18:35:37     remote.log.manager.task.retry.jitter = 0.2
2024-11-26 18:35:37     remote.log.manager.thread.pool.size = 10
2024-11-26 18:35:37     remote.log.metadata.custom.metadata.max.bytes = 128
2024-11-26 18:35:37     remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
2024-11-26 18:35:37     remote.log.metadata.manager.class.path = null
2024-11-26 18:35:37     remote.log.metadata.manager.impl.prefix = rlmm.config.
2024-11-26 18:35:37     remote.log.metadata.manager.listener.name = null
2024-11-26 18:35:37     remote.log.reader.max.pending.tasks = 100
2024-11-26 18:35:37     remote.log.reader.threads = 10
2024-11-26 18:35:37     remote.log.storage.manager.class.name = null
2024-11-26 18:35:37     remote.log.storage.manager.class.path = null
2024-11-26 18:35:37     remote.log.storage.manager.impl.prefix = rsm.config.
2024-11-26 18:35:37     remote.log.storage.system.enable = false
2024-11-26 18:35:37     replica.fetch.backoff.ms = 1000
2024-11-26 18:35:37     replica.fetch.max.bytes = 1048576
2024-11-26 18:35:37     replica.fetch.min.bytes = 1
2024-11-26 18:35:37     replica.fetch.response.max.bytes = 10485760
2024-11-26 18:35:37     replica.fetch.wait.max.ms = 500
2024-11-26 18:35:37     replica.high.watermark.checkpoint.interval.ms = 5000
2024-11-26 18:35:37     replica.lag.time.max.ms = 30000
2024-11-26 18:35:37     replica.selector.class = null
2024-11-26 18:35:37     replica.socket.receive.buffer.bytes = 65536
2024-11-26 18:35:37     replica.socket.timeout.ms = 30000
2024-11-26 18:35:37     replication.quota.window.num = 11
2024-11-26 18:35:37     replication.quota.window.size.seconds = 1
2024-11-26 18:35:37     request.timeout.ms = 30000
2024-11-26 18:35:37     reserved.broker.max.id = 1000
2024-11-26 18:35:37     sasl.client.callback.handler.class = null
2024-11-26 18:35:37     sasl.enabled.mechanisms = [GSSAPI]
2024-11-26 18:35:37     sasl.jaas.config = null
2024-11-26 18:35:37     sasl.kerberos.kinit.cmd = /usr/bin/kinit
2024-11-26 18:35:37     sasl.kerberos.min.time.before.relogin = 60000
2024-11-26 18:35:37     sasl.kerberos.principal.to.local.rules = [DEFAULT]
2024-11-26 18:35:37     sasl.kerberos.service.name = null
2024-11-26 18:35:37     sasl.kerberos.ticket.renew.jitter = 0.05
2024-11-26 18:35:37     sasl.kerberos.ticket.renew.window.factor = 0.8
2024-11-26 18:35:37     sasl.login.callback.handler.class = null
2024-11-26 18:35:37     sasl.login.class = null
2024-11-26 18:35:37     sasl.login.connect.timeout.ms = null
2024-11-26 18:35:37     sasl.login.read.timeout.ms = null
2024-11-26 18:35:37     sasl.login.refresh.buffer.seconds = 300
2024-11-26 18:35:37     sasl.login.refresh.min.period.seconds = 60
2024-11-26 18:35:37     sasl.login.refresh.window.factor = 0.8
2024-11-26 18:35:37     sasl.login.refresh.window.jitter = 0.05
2024-11-26 18:35:37     sasl.login.retry.backoff.max.ms = 10000
2024-11-26 18:35:37     sasl.login.retry.backoff.ms = 100
2024-11-26 18:35:37     sasl.mechanism.controller.protocol = GSSAPI
2024-11-26 18:35:37     sasl.mechanism.inter.broker.protocol = GSSAPI
2024-11-26 18:35:37     sasl.oauthbearer.clock.skew.seconds = 30
2024-11-26 18:35:37     sasl.oauthbearer.expected.audience = null
2024-11-26 18:35:37     sasl.oauthbearer.expected.issuer = null
2024-11-26 18:35:37     sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
2024-11-26 18:35:37     sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
2024-11-26 18:35:37     sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
2024-11-26 18:35:37     sasl.oauthbearer.jwks.endpoint.url = null
2024-11-26 18:35:37     sasl.oauthbearer.scope.claim.name = scope
2024-11-26 18:35:37     sasl.oauthbearer.sub.claim.name = sub
2024-11-26 18:35:37     sasl.oauthbearer.token.endpoint.url = null
2024-11-26 18:35:37     sasl.server.callback.handler.class = null
2024-11-26 18:35:37     sasl.server.max.receive.size = 524288
2024-11-26 18:35:37     security.inter.broker.protocol = PLAINTEXT
2024-11-26 18:35:37     security.providers = null
2024-11-26 18:35:37     server.max.startup.time.ms = 9223372036854775807
2024-11-26 18:35:37     socket.connection.setup.timeout.max.ms = 30000
2024-11-26 18:35:37     socket.connection.setup.timeout.ms = 10000
2024-11-26 18:35:37     socket.listen.backlog.size = 50
2024-11-26 18:35:37     socket.receive.buffer.bytes = 102400
2024-11-26 18:35:37     socket.request.max.bytes = 104857600
2024-11-26 18:35:37     socket.send.buffer.bytes = 102400
2024-11-26 18:35:37     ssl.allow.dn.changes = false
2024-11-26 18:35:37     ssl.allow.san.changes = false
2024-11-26 18:35:37     ssl.cipher.suites = []
2024-11-26 18:35:37     ssl.client.auth = none
2024-11-26 18:35:37     ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
2024-11-26 18:35:37     ssl.endpoint.identification.algorithm = https
2024-11-26 18:35:37     ssl.engine.factory.class = null
2024-11-26 18:35:37     ssl.key.password = null
2024-11-26 18:35:37     ssl.keymanager.algorithm = SunX509
2024-11-26 18:35:37     ssl.keystore.certificate.chain = null
2024-11-26 18:35:37     ssl.keystore.key = null
2024-11-26 18:35:37     ssl.keystore.location = null
2024-11-26 18:35:37     ssl.keystore.password = null
2024-11-26 18:35:37     ssl.keystore.type = JKS
2024-11-26 18:35:37     ssl.principal.mapping.rules = DEFAULT
2024-11-26 18:35:37     ssl.protocol = TLSv1.3
2024-11-26 18:35:37     ssl.provider = null
2024-11-26 18:35:37     ssl.secure.random.implementation = null
2024-11-26 18:35:37     ssl.trustmanager.algorithm = PKIX
2024-11-26 18:35:37     ssl.truststore.certificates = null
2024-11-26 18:35:37     ssl.truststore.location = null
2024-11-26 18:35:37     ssl.truststore.password = null
2024-11-26 18:35:37     ssl.truststore.type = JKS
2024-11-26 18:35:37     telemetry.max.bytes = 1048576
2024-11-26 18:35:37     transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
2024-11-26 18:35:37     transaction.max.timeout.ms = 900000
2024-11-26 18:35:37     transaction.partition.verification.enable = true
2024-11-26 18:35:37     transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
2024-11-26 18:35:37     transaction.state.log.load.buffer.size = 5242880
2024-11-26 18:35:37     transaction.state.log.min.isr = 1
2024-11-26 18:35:37     transaction.state.log.num.partitions = 50
2024-11-26 18:35:37     transaction.state.log.replication.factor = 1
2024-11-26 18:35:37     transaction.state.log.segment.bytes = 104857600
2024-11-26 18:35:37     transactional.id.expiration.ms = 604800000
2024-11-26 18:35:37     unclean.leader.election.enable = false
2024-11-26 18:35:37     unstable.api.versions.enable = false
2024-11-26 18:35:37     unstable.metadata.versions.enable = false
2024-11-26 18:35:37     zookeeper.clientCnxnSocket = null
2024-11-26 18:35:37     zookeeper.connect = zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
2024-11-26 18:35:37     zookeeper.connection.timeout.ms = null
2024-11-26 18:35:37     zookeeper.max.in.flight.requests = 10
2024-11-26 18:35:37     zookeeper.metadata.migration.enable = false
2024-11-26 18:35:37     zookeeper.metadata.migration.min.batch.size = 200
2024-11-26 18:35:37     zookeeper.session.timeout.ms = 18000
2024-11-26 18:35:37     zookeeper.set.acl = false
2024-11-26 18:35:37     zookeeper.ssl.cipher.suites = null
2024-11-26 18:35:37     zookeeper.ssl.client.enable = false
2024-11-26 18:35:37     zookeeper.ssl.crl.enable = false
2024-11-26 18:35:37     zookeeper.ssl.enabled.protocols = null
2024-11-26 18:35:37     zookeeper.ssl.endpoint.identification.algorithm = HTTPS
2024-11-26 18:35:37     zookeeper.ssl.keystore.location = null
2024-11-26 18:35:37     zookeeper.ssl.keystore.password = null
2024-11-26 18:35:37     zookeeper.ssl.keystore.type = null
2024-11-26 18:35:37     zookeeper.ssl.ocsp.enable = false
2024-11-26 18:35:37     zookeeper.ssl.protocol = TLSv1.2
2024-11-26 18:35:37     zookeeper.ssl.truststore.location = null
2024-11-26 18:35:37     zookeeper.ssl.truststore.password = null
2024-11-26 18:35:37     zookeeper.ssl.truststore.type = null
2024-11-26 18:35:37  (kafka.server.KafkaConfig)
2024-11-26 18:35:37 [2024-11-26 09:35:37,375] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,375] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,376] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,377] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,380] INFO [KafkaServer id=2] Rewriting /var/lib/kafka/data/meta.properties (kafka.server.KafkaServer)
2024-11-26 18:35:37 [2024-11-26 09:35:37,415] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,420] INFO No logs found to be loaded in /var/lib/kafka/data (kafka.log.LogManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,425] INFO Loaded 0 logs in 10ms (kafka.log.LogManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,426] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,427] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,434] INFO Starting the log cleaner (kafka.log.LogCleaner)
2024-11-26 18:35:37 [2024-11-26 09:35:37,533] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,567] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,575] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
2024-11-26 18:35:37 [2024-11-26 09:35:37,589] INFO [zk-broker-2-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,753] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
2024-11-26 18:35:37 [2024-11-26 09:35:37,764] INFO [SocketServer listenerType=ZK_BROKER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
2024-11-26 18:35:37 [2024-11-26 09:35:37,765] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
2024-11-26 18:35:37 [2024-11-26 09:35:37,766] INFO [SocketServer listenerType=ZK_BROKER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
2024-11-26 18:35:37 [2024-11-26 09:35:37,772] INFO [zk-broker-2-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,790] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,790] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,791] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,792] INFO [ExpirationReaper-2-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,792] INFO [ExpirationReaper-2-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,805] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
2024-11-26 18:35:37 [2024-11-26 09:35:37,805] INFO [AddPartitionsToTxnSenderThread-2]: Starting (kafka.server.AddPartitionsToTxnManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,824] INFO Creating /brokers/ids/2 (is it secure? false) (kafka.zk.KafkaZkClient)
2024-11-26 18:35:37 [2024-11-26 09:35:37,837] INFO Stat of the created znode at /brokers/ids/2 is: 25,25,1732613737832,1732613737832,1,0,0,72057594391363584,262,0,25
2024-11-26 18:35:37  (kafka.zk.KafkaZkClient)
2024-11-26 18:35:37 [2024-11-26 09:35:37,837] INFO Registered broker 2 at path /brokers/ids/2 with addresses: PLAINTEXT://kafka-2:9092,PLAINTEXT_HOST://localhost:39092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)
2024-11-26 18:35:37 [2024-11-26 09:35:37,855] INFO [ControllerEventThread controllerId=2] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,859] INFO [ExpirationReaper-2-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,864] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,865] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,868] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
2024-11-26 18:35:37 [2024-11-26 09:35:37,872] INFO [Controller id=2] 2 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,873] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
2024-11-26 18:35:37 [2024-11-26 09:35:37,876] INFO [Controller id=2] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map()) (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,876] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
2024-11-26 18:35:37 [2024-11-26 09:35:37,881] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
2024-11-26 18:35:37 [2024-11-26 09:35:37,887] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
2024-11-26 18:35:37 [2024-11-26 09:35:37,893] INFO [TxnMarkerSenderThread-2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,893] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
2024-11-26 18:35:37 [2024-11-26 09:35:37,906] INFO [MetadataCache brokerId=2] Updated cache from existing None to latest Features(version=3.7-IV4, finalizedFeatures={}, finalizedFeaturesEpoch=0). (kafka.server.metadata.ZkMetadataCache)
2024-11-26 18:35:37 [2024-11-26 09:35:37,906] INFO [Controller id=2] Registering handlers (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,912] INFO [Controller id=2] Deleting log dir event notifications (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,915] INFO [Controller id=2] Deleting isr change notifications (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,918] INFO [Controller id=2] Initializing controller context (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,924] INFO [ExpirationReaper-2-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
2024-11-26 18:35:37 [2024-11-26 09:35:37,929] INFO [Controller id=2] Initialized broker epochs cache: HashMap(2 -> 25) (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,933] DEBUG [Controller id=2] Register BrokerModifications handler for Set(2) (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,946] DEBUG [Channel manager on controller 2]: Controller 2 trying to connect to broker 2 (kafka.controller.ControllerChannelManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,950] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,956] INFO [RequestSendThread controllerId=2] Starting (kafka.controller.RequestSendThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,957] INFO [Controller id=2] Currently active brokers in the cluster: Set(2) (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,958] INFO [Controller id=2] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,958] INFO [Controller id=2] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,959] INFO [Controller id=2] Fetching topic deletions in progress (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,963] INFO [Controller id=2] List of topics to be deleted:  (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,963] INFO [Controller id=2] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,963] INFO [Controller id=2] Initializing topic deletion manager (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,963] INFO [Topic Deletion Manager 2] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
2024-11-26 18:35:37 [2024-11-26 09:35:37,966] INFO [Controller id=2] Sending update metadata request (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,970] INFO [SocketServer listenerType=ZK_BROKER, nodeId=2] Enabling request processing. (kafka.network.SocketServer)
2024-11-26 18:35:37 [2024-11-26 09:35:37,970] INFO [Controller id=2 epoch=1] Sending UpdateMetadata request to brokers HashSet(2) for 0 partitions (state.change.logger)
2024-11-26 18:35:37 [2024-11-26 09:35:37,975] INFO Awaiting socket connections on 0.0.0.0:39092. (kafka.network.DataPlaneAcceptor)
2024-11-26 18:35:37 [2024-11-26 09:35:37,976] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
2024-11-26 18:35:37 [2024-11-26 09:35:37,977] INFO [ReplicaStateMachine controllerId=2] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
2024-11-26 18:35:37 [2024-11-26 09:35:37,978] INFO [ReplicaStateMachine controllerId=2] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
2024-11-26 18:35:37 [2024-11-26 09:35:37,980] INFO Kafka version: 7.7.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
2024-11-26 18:35:37 [2024-11-26 09:35:37,981] INFO Kafka commitId: 91d86f33092378c89731b4a9cf1ce5db831a2b07 (org.apache.kafka.common.utils.AppInfoParser)
2024-11-26 18:35:37 [2024-11-26 09:35:37,981] INFO Kafka startTimeMs: 1732613737977 (org.apache.kafka.common.utils.AppInfoParser)
2024-11-26 18:35:37 [2024-11-26 09:35:37,983] INFO [RequestSendThread controllerId=2] Controller 2 connected to kafka-2:9092 (id: 2 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
2024-11-26 18:35:37 [2024-11-26 09:35:37,983] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)
2024-11-26 18:35:37 [2024-11-26 09:35:37,985] INFO [ReplicaStateMachine controllerId=2] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
2024-11-26 18:35:37 [2024-11-26 09:35:37,987] DEBUG [ReplicaStateMachine controllerId=2] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
2024-11-26 18:35:37 [2024-11-26 09:35:37,987] INFO [PartitionStateMachine controllerId=2] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
2024-11-26 18:35:37 [2024-11-26 09:35:37,988] INFO [PartitionStateMachine controllerId=2] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
2024-11-26 18:35:37 [2024-11-26 09:35:37,989] DEBUG [PartitionStateMachine controllerId=2] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
2024-11-26 18:35:37 [2024-11-26 09:35:37,989] INFO [Controller id=2] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,994] INFO [Controller id=2] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,995] INFO [Controller id=2] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,995] INFO [Controller id=2] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,995] INFO [Controller id=2] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
2024-11-26 18:35:37 [2024-11-26 09:35:37,996] INFO [Controller id=2] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
2024-11-26 18:35:38 [2024-11-26 09:35:38,000] INFO [Controller id=2] Starting the controller scheduler (kafka.controller.KafkaController)
2024-11-26 18:35:38 [2024-11-26 09:35:38,023] TRACE [Controller id=2 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 0 sent to broker kafka-2:9092 (id: 2 rack: null) (state.change.logger)
2024-11-26 18:35:38 [2024-11-26 09:35:38,076] INFO [zk-broker-2-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node kafka-2:9092 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
2024-11-26 18:35:38 [2024-11-26 09:35:38,098] INFO [zk-broker-2-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node kafka-2:9092 (id: 2 rack: null) (kafka.server.NodeToControllerRequestThread)
2024-11-26 18:35:40 [2024-11-26 09:35:40,579] INFO Creating topic _schemas with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(2)) (kafka.zk.AdminZkClient)
2024-11-26 18:35:40 [2024-11-26 09:35:40,589] INFO [Controller id=2] New topics: [Set(_schemas)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(_schemas,Some(0VsttPfISd2NmM8-fX8LXQ),Map(_schemas-0 -> ReplicaAssignment(replicas=2, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
2024-11-26 18:35:40 [2024-11-26 09:35:40,590] INFO [Controller id=2] New partition creation callback for _schemas-0 (kafka.controller.KafkaController)
2024-11-26 18:35:40 [2024-11-26 09:35:40,591] INFO [Controller id=2 epoch=1] Changed partition _schemas-0 state from NonExistentPartition to NewPartition with assigned replicas 2 (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,591] INFO [Controller id=2 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,593] TRACE [Controller id=2 epoch=1] Changed state of replica 2 for partition _schemas-0 from NonExistentReplica to NewReplica (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,593] INFO [Controller id=2 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,603] INFO [Controller id=2 epoch=1] Changed partition _schemas-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=2, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,604] TRACE [Controller id=2 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2], partitionEpoch=0, replicas=[2], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 2 for partition _schemas-0 (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,604] INFO [Controller id=2 epoch=1] Sending LeaderAndIsr request to broker 2 with 1 become-leader and 0 become-follower partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,608] INFO [Controller id=2 epoch=1] Sending UpdateMetadata request to brokers HashSet(2) for 1 partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,609] TRACE [Controller id=2 epoch=1] Changed state of replica 2 for partition _schemas-0 from NewReplica to OnlineReplica (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,609] INFO [Controller id=2 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,614] INFO [Broker id=2] Handling LeaderAndIsr request correlationId 1 from controller 2 for 1 partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,615] TRACE [Broker id=2] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2], partitionEpoch=0, replicas=[2], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 1 from controller 2 epoch 1 (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,621] TRACE [Broker id=2] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _schemas-0 (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,622] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
2024-11-26 18:35:40 [2024-11-26 09:35:40,622] INFO [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 2 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,641] INFO [LogLoader partition=_schemas-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
2024-11-26 18:35:40 [2024-11-26 09:35:40,644] INFO Created log for partition _schemas-0 in /var/lib/kafka/data/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
2024-11-26 18:35:40 [2024-11-26 09:35:40,645] INFO [Partition _schemas-0 broker=2] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
2024-11-26 18:35:40 [2024-11-26 09:35:40,645] INFO [Partition _schemas-0 broker=2] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
2024-11-26 18:35:40 [2024-11-26 09:35:40,646] INFO [Broker id=2] Leader _schemas-0 with topic id Some(0VsttPfISd2NmM8-fX8LXQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [2], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1. (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,649] TRACE [Broker id=2] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _schemas-0 (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,650] INFO [Broker id=2] Finished LeaderAndIsr request in 37ms correlationId 1 from controller 2 for 1 partitions (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,652] TRACE [Controller id=2 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=0VsttPfISd2NmM8-fX8LXQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 1 sent to broker kafka-2:9092 (id: 2 rack: null) (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,654] TRACE [Broker id=2] Cached leader info UpdateMetadataPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2], zkVersion=0, replicas=[2], offlineReplicas=[]) for partition _schemas-0 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,654] INFO [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
2024-11-26 18:35:40 [2024-11-26 09:35:40,656] TRACE [Controller id=2 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 2 sent to broker kafka-2:9092 (id: 2 rack: null) (state.change.logger)
2024-11-26 18:35:43 [2024-11-26 09:35:43,002] INFO [Controller id=2] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
2024-11-26 18:35:43 [2024-11-26 09:35:43,002] TRACE [Controller id=2] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
2024-11-26 18:35:43 [2024-11-26 09:35:43,005] DEBUG [Controller id=2] Topics not in preferred replica for broker 2 Map() (kafka.controller.KafkaController)
2024-11-26 18:35:43 [2024-11-26 09:35:43,006] TRACE [Controller id=2] Leader imbalance ratio for broker 2 is 0.0 (kafka.controller.KafkaController)


```


# 
https://velog.io/@hyun6ik/Apache-Kafka-In-Sync-Replicas


cf) confluent hardware 정보
https://docs.confluent.io/platform/current/kafka/deployment.html#cp-production-parameters